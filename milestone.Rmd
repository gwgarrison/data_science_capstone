---
title: "Data Science Capstone Milestone Report"
author: "Gary Garrison"
date: "March 19, 2015"
output: html_document
---

## Preprocessing

The data for this project was downloaded from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip. The following code of consists of preprocessing steps that will facilitate tokenization, exploratory analysis, and future modeling of the text data. Since profanity is such a subjective thing, I used the "Seven Dirty Words" (Wikipedia 2015) from George Carlin. I also added some derivatives of the original seven.

```{r echo=FALSE}
library(tm);library(SnowballC);#library(qdap);library(qdapDictionaries);
library(plyr);library(RColorBrewer);library(ggplot2); library(scales);library(wordcloud)
library(RWeka);library(slam);library(dplyr)
# setting this prevents an error in RWeka package
options(mc.cores=1)
```

```{r}
blogs <- readLines("samples/blogs.txt")
news <- readLines("samples/news.txt")
twitter <- readLines("samples/twitter.txt")

# combine documents and create the corpus
doc <- c(blogs,news,twitter)
crps <- Corpus(VectorSource(doc), readerControl = list(language="english"))

# get rid of the numbers and punctuation
crps <- tm_map(crps,removeNumbers)
crps <- tm_map(crps,removePunctuation)

# remove profanity
profanity.df <- read.table("profanity.txt")
profanity <- as.character(profanity.df[,1])
crps <- tm_map(crps,removeWords,profanity)

# function to get rid of non standard characters
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
crps <- tm_map(crps, toSpace, "/|@|\\||\\)|\\(")

# change to all lower case
crps <- tm_map(crps, content_transformer(tolower))

# remove extra whitespace
crps <- tm_map(crps,stripWhitespace)
```

The full corpora had the follwing characteristics:

Line, word, and byte counts:
```{r,echo=TRUE}
system("wc  ./final/en_US/* > full_wc.txt")
full_wc <- read.table("full_wc.txt")
names(full_wc) <- c("Lines","Words","Bytes","File.Name")
print(full_wc,row.names = FALSE)
```
The wc command returns the number of lines, words, and bytes for each of the three source files. In the output above the the blogs data has 899,288 lines, 37,334,626 words, and 210,159,817 bytes. This is lot of data so for our model building and exploratory analysis we use only sample of this data

We created a function to sample to get a sample from each of the three types which included news group, blog, and Twitter data. 

After sampling our data had the following characteristics:
```{r, echo=TRUE}
system("wc  ./samples/*")
system("wc  ./samples/* > sample_wc.txt")
sample_wc <- read.table("sample_wc.txt")
names(sample_wc) <- c("Lines","Words","Bytes","File.Name")
print(sample_wc,row.names = FALSE)
```

## Exploratory Analysis

### Statistical Summaries
Vocabulary size of 56,519 terms after sampling
```{r}
dtm <- DocumentTermMatrix(crps)
# remove sparse terms so subsequent matrix conversions to do not error
d <- removeSparseTerms(dtm,.9999)
freq <- sort(colSums(as.matrix(d)), decreasing=TRUE)
wf   <- data.frame(word=names(freq), freq=freq)
BigramTokenizer <- function(x) RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))
bdm <- TermDocumentMatrix(crps, control = list(tokenize = BigramTokenizer))
tdm <- TermDocumentMatrix(crps, control = list(tokenize = TrigramTokenizer))
udm <- TermDocumentMatrix(crps)
dm <- removeSparseTerms(bdm, 0.9999)
tm <- removeSparseTerms(tdm, 0.9999)
um <- removeSparseTerms(udm, 0.9999)
tm <- as.matrix(tm)
dm <- as.matrix(dm)
um <- as.matrix(um)
b.freq <- as.data.frame(rowSums(dm))
t.freq <- as.data.frame(rowSums(tm))
u.freq <- as.data.frame(rowSums(um))
b.freq$bigram <- row.names(b.freq)
t.freq$trigram <- row.names(t.freq)
u.freq$unigram <- row.names(u.freq)
uni.freq <- tbl_df(data.frame(u.freq[,2],u.freq[,1]))
bi.freq <- tbl_df(data.frame(b.freq[,2],b.freq[,1]))
tri.freq <- tbl_df(data.frame(t.freq[,2],t.freq[,1]))
names(bi.freq) <- c("bigram","count")
names(tri.freq) <- c("trigram","count")
names(uni.freq) <- c("unigram","count")
```
20 most common words in sample corpus:
```{r}
head(freq,20)
```


### Exploratory Plots
Bar plot of most common words. The axis is flipped and reordered by the count (Zumel 2014)
```{r most_frequent,fig.width=8,fig.height=5}
wf.ordered <- transform(wf,word = reorder(word,freq))
p <- ggplot(subset(wf.ordered, freq>2000), aes(word, freq))
p <- p + geom_bar(stat="identity",fill = "blue")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) + coord_flip() +
  xlab("Words") + ylab("Count") + ggtitle("Unigram/Word Count for Sample Corpus")
p
```

Bar plot of most common bigrams. 
```{r most_frequent_bigrams,fig.width=8,fig.height=5}
bf.ordered <- transform(bi.freq,bigram = reorder(bigram ,count))
p <- ggplot(subset(bf.ordered, count>800), aes(bigram, count))
p <- p + geom_bar(stat="identity",fill = "red")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) + coord_flip() +
  xlab("Bigrams") + ylab("Count") + ggtitle("Bigram Count for Sample Corpus")
p
```


The word cloud below gives a quick visual representation of word frequency:
```{r wordcloud,fig.width= 7,fig.height= 7,fig.align='center',fig.cap="Word Cloud of Most Frequent Words"}
# word cloud
set.seed(777)
wordcloud(names(freq), freq, min.freq=900,scale = c(4,1), colors=brewer.pal(6, "Dark2"),
          main = "Most Frequently Used Words in Sample Corpus")
```


## Findings
We can see from the word frequencies and bigram frequencies that "stop" words dominate the corpus as one would expect. For some text processing tasks these words are removed, but for our next word prediction we will leave them in place at least for our initial modeling.


## Plan for Prediction and App Creation
My plan is to create a simple ngram model for next word prediction. At this point I plan to use trigrams and backoff to bigram or unigram models if the trigram does not result in a good prediction. If accuracy is poor, I will investigate more sophisticated actions such as interpolation and smoothing algorithms such as Good-Turing or Kneser-Ney. (Jurafsky 2015)

## References

Wikipedia (2015). Seven dirty words. URL http://en.wikipedia.org/wiki/Seven_dirty_words.

Zumel,Nina. Mount, John. (2014). Practical Data Science with R (Kindle Location 1262). Manning Publications. Kindle Edition. 

Jurafsky, Dan. Manning, Christopher. (2014). Natural Language Processing (Coursera). URL https://class.coursera.org/nlp/lecture/preview
 
All code for this project can be found at the following Github Repoistory:
https://github.com/gwgarrison/data_science_capstone